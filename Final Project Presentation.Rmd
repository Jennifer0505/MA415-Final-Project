---
title: "Final Project Presentation"
author: "Xuetong Ma"
date: "2017/12/18"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

##Introduction

In order to figure out what Internet users are talking about and how they feel about California fire, this report analyzed data collected from twitter a week after the fire happened, mainly focus on:
  
  1.The frequency of words mentioned by users, showing by Word Cloud.
  
  2.Visualization of sentiments towards the fire among different hashtags and different locations
  
  3.Statistical analysis to generate the population.
  
```{r package setup, message=FALSE, warning=FALSE, include=FALSE}

require(devtools)
require(ggmap)
require(plyr)
require(stringr)
require(dplyr)
require(ggplot2)
require(reshape)
require(tm)
require(RJSONIO)
require(wordcloud)
require(grid)
require(gridExtra)
require(tidyr)
require(tidyverse)
require(tidytext)
require(lubridate)
require(plyr)
#require(shinyIncubator)
require(shiny)
require(maps)
require(leaflet)
require(rsconnect)

```

## Data summary

Total data gathered:7500, 2500 under each set.
After gathering location and set the scope to the US, reduced to:
1288 observations for #Californiafire, 
876 observations for "California fire" 
and 1256 observations for #Californiawildfires, together 3240 observations.

```{r read data, message=FALSE, warning=FALSE, include=FALSE}

total <- read.csv("total.csv",row.names = 1)
data3 <- read.csv("data3.csv",row.names = 1)
data2 <- read.csv("data2.csv", row.names = 1)
data1 <- read.csv("data1.csv",row.names = 1)
```

```{r text clean, message=FALSE, warning=FALSE, include=FALSE}
#Define text clean function
CleanTweets <- function(tweets)
  {
    tweets = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets)
    tweets = gsub("@\\w+", "", tweets)
    tweets = gsub("[[:punct:]]", "", tweets)
    tweets = gsub("[[:digit:]]", "", tweets)
    tweets = gsub("http\\w+", "", tweets)
    tweets = gsub("[ \t]{2,}", "", tweets)
    tweets = gsub("^\\s+|\\s+$", "", tweets)
    tweets = gsub("amp", "", tweets)
    # define "tolower error handling" function
    try.tolower = function(x)
    {
      y = NA
      try_error = tryCatch(tolower(x), error=function(e) e)
      if (!inherits(try_error, "error"))
        y = tolower(x)
      return(y)
    }
    
    tweets = sapply(tweets, try.tolower)
    tweets = tweets[tweets != ""]
    names(tweets) = NULL
    return(tweets)
  }

#Clean all the text
tweets1 <- gettext(data1$text)  # tweets of "#Californiafire"
tweets2 <- gettext(data2$text)  # tweets of "California fire"
tweets3 <- gettext(data3$text)  # tweets of "#Californiawildfire"
tweets4 <- gettext(total$text)  # all tweets together

```

```{r wordcloud function, message=FALSE, warning=FALSE, include=FALSE}

  wordcloudentity<-function(tweets)
  {
    tweetCorpus<-Corpus(VectorSource(CleanTweets(tweets)))
    tweetTDM<-TermDocumentMatrix(tweetCorpus,control=list(removePunctuation=TRUE
    ,stopwords=c("California", "fire", "wild",stopwords('english')), 
    removeNumbers=TRUE,tolower=TRUE))
    tdMatrix <- as.matrix(tweetTDM) # creating a data matrix
    
    sortedMatrix<-sort(rowSums(tdMatrix),decreasing=TRUE) # calculate row sum of                       each term and sort in descending order (high freq to low)
   cloudFrame<-data.frame(word=names(sortedMatrix),freq=sortedMatrix)#extracting
   #names from named list in prev command and binding together into a dataframe 
   #with frequencies
     wcloudentity<-wordcloud(cloudFrame$word,cloudFrame$freq,max.words=200, colors=brewer.pal(8,"Dark2"),scale=c(8,1), random.order=FALSE)
 return(wcloudentity)
  }
```

## Wordcloud for total data

```{r wordcloud total, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for total data
wordtotal <- wordcloudentity(tweets4)

```



## Wordcloud for hashtag #California fire
```{r wordcloud1, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for #Californiafire
wordcloud1 <- wordcloudentity(tweets1)

```
The first wordcloud for #California fire is approximately neutral


## Wordcloud for key words "California fire"
 Tweets under key words "California fire" were more likely to be negative, invoving words like illegal, criminal.

```{r wordcloud2, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for "California fire""
wordcloud2 <- wordcloudentity(tweets2)

```
 Tweets under key words "California fire" were more likely to be negative, invoving words like illegal, criminal.


## Wordcloud for hashtag #California wild fires
On the contrast, tweets under this hashtag are more positive, with words like brave and bless.
```{r wordcloud3, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for #Californiafire
wordcloud3 <- wordcloudentity(tweets3)

```


##Histogram of sentiment

The proportion of negative words is larger, and their sentiment are stronger, which means people are more likely to complain for the fire instead of praying for the fire.
```{r sentiment plot, echo=FALSE, message=FALSE, warning=FALSE}

#remove some "stopwords"
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- total %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(tweets = text) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
#find the most common words in tweets
commonword <- tweet_words %>%
  dplyr::count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  coord_flip()

bing <- get_sentiments("bing")
bing_word_counts <- tweet_words %>%
  inner_join(bing) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts <- bing_word_counts[-1, ]

#remove the top key words and then plot words
bing_word_counts %>%
  filter(n > 20) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Sentiment")
```


## Map with sentiment

Here red represents positive sentiment and blue represent regative sentiment.
There seems to be more users located on the east coast than on the west coast. Also, the color for tweets from the west coast is darker, especially in south California which means people there have more negative sentiment.
```{r map sentiment, echo=FALSE, message=FALSE, warning=FALSE}
#Get the US map
#######If this line doesn't run or shows an error message, just run it again please, sometimes it happens but will eliminate after rerunning it#############
USmap = ggmap(get_googlemap(center =as.numeric(geocode("United States")),
                            scale=2, zoom=4), extent="device") 

#Draw map for total data
map1<- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((total$score>=0)),"red", "blue"), data=total, alpha=0.4, size=total$absolute_score) +
  scale_size_continuous(range=total$score)+
  ggtitle("Sentiment map for total data set")
map1
```


## Mapping under #Californiafire
```{r map2, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under #Californiafire
map2 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data1$score>=0)),"red", "blue"), data=data1, alpha=0.4, size=data1$absolute_score) +
  scale_size_continuous(range=data1$score)+
  ggtitle("Sentiment map for #Californiafire")
map2
```


## Mapping under 'California fire'
```{r map3, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under 'California fire'
map3 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data2$score>=0)),"red", "blue"), data=data2, alpha=0.4, size=data2$absolute_score) +
  scale_size_continuous(range=data2$score)+
  ggtitle("Sentiment map for 'California fire'")
map3
```


## Mapping under #California wild fire
```{r map4, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under #California wild fire
map4 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data3$score>=0)),"red", "blue"), data=data3, alpha=0.4, size=data3$absolute_score) +
  scale_size_continuous(range=data3$score)+
  ggtitle("Sentiment map for #California wild fire")
map4
```



##Shiny Interactive Map

  As we already seen the sentiment scroe on the map, the interactive map focus 
on the number of retweets counts of each tweets to generate the popularity of the twitter. In the map, the deeper the color of the popup points, the more popular the tweet content is.
 
 
 You can zoom the scale of the map and click on every individual points to find out more detail about that tweet, like sentiment socre, user name and retweet cout. Also, you can discover whether retweet counts is related to position of the user. 
 Below is the link for the application:
https://sabrina414.shinyapps.io/InteractiveMap/


##Statistical Analysis
Summary table
```{r score summary, echo=FALSE, message=FALSE, warning=FALSE}
summary(total$score)
```

 From the summary of score we can see the average sentiment score is negative, with minimum of -4 and maximum of 5. This means the overall sentiment is more negative, which is the same conclusion as above.
 

##Test of normality

```{r normal analyze, echo=FALSE, message=FALSE, warning=FALSE}

#draw histogram to see whether it follows normal distribution
hist(total$score)
```

The histogram shows that the data is approximately normal distributed


##ANOVA table
```{r anova, echo=FALSE, message=FALSE, warning=FALSE}
# Anova table that analyze the relationship between sentiment and retweet number
summary(lm(total$retweetCount~total$absolute_score))

```
Conclusion is that sentiment score have effect on retweet count, the stronger sentiment is, the more retweet count it would cause.

##Smooth Line

The smooth line confirms the conclusion above. However, the dramatic trend may indicate specific relationship between retweet count and sentiment score, which need future investigation.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#Smooth trend
ggplot(total) +
  geom_smooth(mapping = aes(x = total$score, y = total$retweetCount))+
  ggtitle("Smooth Line for Retweet count v.s Sentiment score") + 
  xlab("Sentiment score") +
  ylab("Retweet count")
```

##Is location matter?
Both latitude and longitude are highly insignificant in the anova table below, which means that the strength of attitude is not so related with people's location.

```{r, echo=FALSE}
summary(lm(total$absolute_score~total$lat+total$lon))
```


##Conclusion
 Conclusion:
 1.The sentiment of texts is different among different keywords and hashtags. Overall the data set is more negative. 
 
 2.The popularity of the text, represented by retweet counts, is related to the strength of attitude.
 
 3.Location doesn't matter for sentiment score

##Improvement:
 There might be some repetitive texts among these three dataset since the keywords and hashtags are quite similar. these repetitive texts should be removed. 
 
 
Also, it would be better to overcome the restrict of Google geocode API and gather more data. A larger dataset will saturate this project with more factual evidences.