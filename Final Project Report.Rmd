---
title: "Final Project Report"
author: "Xuetong Ma"
date: "2017/12/14"
output: html_document
---

#Introduction

  The Thomas Fire has last for 14 days, ravaged Southern California north of Los Angeles and began to bear down on affluent swaths of Santa Barbara and Montecito these days.According to the National Weather Service, a red flag warning remained in effect in both the mountains in Santa Barbara County and along the South Coast with humidity dropping to the teens and wind gusts topping 55 mph overnight.
  In order to figure out what Internet users are talking about and how they feel about this disaster, this report analyzed data collected from twitter a week after the fire happened, due to the fact that Twitter API only allows people to access user data from very recent seven days for confidential reasons. This report mainly focus on three parts:

1.The frequency of words mentioned by users, showing by Word Cloud.

2.Visualization of sentiments towards the fire among different hashtags and different locations: whether people are more likely to complain or encourage each other when facing disaster. Besides normal maps, there is also a shiny application which creates an interactive map on tweet popularity.

3.Statistical analysis that whether there is relationship between retweet number and sentiment score, generated from tweet contents using Shapiro.test. Also, there would be an ANOVA table of whether location is influential on sentiment scores, i.e., people live in west may have higher absolute sentiment score than people live in the east.




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package setup, message=FALSE, warning=FALSE, include=FALSE}

require(devtools)
require(ggmap)
require(plyr)
require(stringr)
require(dplyr)
require(ggplot2)
require(reshape)
require(tm)
require(RJSONIO)
require(wordcloud)
require(grid)
require(gridExtra)
require(tidyr)
require(tidyverse)
require(tidytext)
require(lubridate)
require(plyr)
#require(shinyIncubator)
require(shiny)
require(maps)
require(leaflet)
require(rsconnect)

```

#Data summary

  At first I was trying to collect over 15000 data from twitter among three different keywords using searchTwitter() finction: two hashtag #Californiafire, #Californiawildfires and the keyword â€œCalifrnia fire". However, the geocode by Google API restricted 2500 requests per day for non-business use. Therefore, I decrease total data to 7500, with 2500 observations for each topic. After omitting NA and set the scope to the US, I got 1288 observations for #Californiafire, 876 observations for "California fire" and 1256 observations for #Californiawildfires, together 3240 observations.
```{r read data, message=FALSE, warning=FALSE, include=FALSE}

total <- read.csv("total.csv",row.names = 1)
data3 <- read.csv("data3.csv",row.names = 1)
data2 <- read.csv("data2.csv", row.names = 1)
data1 <- read.csv("data1.csv",row.names = 1)
```


```{r text clean, message=FALSE, warning=FALSE, include=FALSE}
#Define text clean function
CleanTweets <- function(tweets)
  {
    tweets = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets)
    tweets = gsub("@\\w+", "", tweets)
    tweets = gsub("[[:punct:]]", "", tweets)
    tweets = gsub("[[:digit:]]", "", tweets)
    tweets = gsub("http\\w+", "", tweets)
    tweets = gsub("[ \t]{2,}", "", tweets)
    tweets = gsub("^\\s+|\\s+$", "", tweets)
    tweets = gsub("amp", "", tweets)
    # define "tolower error handling" function
    try.tolower = function(x)
    {
      y = NA
      try_error = tryCatch(tolower(x), error=function(e) e)
      if (!inherits(try_error, "error"))
        y = tolower(x)
      return(y)
    }
    
    tweets = sapply(tweets, try.tolower)
    tweets = tweets[tweets != ""]
    names(tweets) = NULL
    return(tweets)
  }

#Clean all the text
tweets1 <- gettext(data1$text)  # tweets of "#Californiafire"
tweets2 <- gettext(data2$text)  # tweets of "California fire"
tweets3 <- gettext(data3$text)  # tweets of "#Californiawildfire"
tweets4 <- gettext(total$text)  # all tweets together

```


  To have a general understanding on what are the most popular words that people use in tweets to express their thoughts of the fire, below are the wordclouds under each topic and the total dataframe.
  
  
```{r wordcloud function, message=FALSE, warning=FALSE, include=FALSE}

  wordcloudentity<-function(tweets)
  {
    tweetCorpus<-Corpus(VectorSource(CleanTweets(tweets)))
    tweetTDM<-TermDocumentMatrix(tweetCorpus,control=list(removePunctuation=TRUE
    ,stopwords=c("California", "fire", "wild",stopwords('english')), 
    removeNumbers=TRUE,tolower=TRUE))
    tdMatrix <- as.matrix(tweetTDM) # creating a data matrix
    
    sortedMatrix<-sort(rowSums(tdMatrix),decreasing=TRUE) # calculate row sum of                       each term and sort in descending order (high freq to low)
   cloudFrame<-data.frame(word=names(sortedMatrix),freq=sortedMatrix)#extracting
   #names from named list in prev command and binding together into a dataframe 
   #with frequencies
     wcloudentity<-wordcloud(cloudFrame$word,cloudFrame$freq,max.words=200, colors=brewer.pal(8,"Dark2"),scale=c(8,1), random.order=FALSE)
 return(wcloudentity)
  }
```


From the word cloud for total dataframe, we can see both positive sentiment, like brave, and negative sentiment, like homeless and criminal. While "trump" was also a popular word, which is quite interesting.
```{r wordcloud total, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for total data
wordtotal <- wordcloudentity(tweets4)

```


Wordcloud for hashtag #California fire
```{r wordcloud1, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for #Californiafire
wordcloud1 <- wordcloudentity(tweets1)

```


Wordcloud for key words "California fire"
```{r wordcloud2, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for "California fire""
wordcloud2 <- wordcloudentity(tweets2)

```


Wordcloud for hashtag #California wild fires
```{r wordcloud3, echo=FALSE, message=FALSE, warning=FALSE}
#Wordcloud for #Californiafire
wordcloud3 <- wordcloudentity(tweets3)

```


  From the above three wordcloud for each topic we can see, the first wordcloud for #California fire is approximately neutral, and the second wordcloud shows that tweets under key words "California fire" were more likely to be negative, invoving words like illegal, criminal. On the contrast, the third wordcloud for #California wild fire shows that twitters under this hashtag are more positive, with words like brave and bless.



  Since the three wordcloud for each data set has different sentiment tendency, it is hard to say the sentiment of overall data is positive or negative. Then I wrote a sentiment score function to calculate the score of every text. Text with negative sentiment has negative socre, and the higher absloute value of score, the stronger sentiment it has.
  Below is the histogram for the overall data, we can see that the proportion of negative words is larger, and their sentiment are stronger, which means people are more likely to complain for the fire instead of praying for the fire.
```{r sentiment plot, echo=FALSE, message=FALSE, warning=FALSE}

#remove some "stopwords"
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- total %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(tweets = text) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
#find the most common words in tweets
commonword <- tweet_words %>%
  dplyr::count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  coord_flip()

bing <- get_sentiments("bing")
bing_word_counts <- tweet_words %>%
  inner_join(bing) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts <- bing_word_counts[-1, ]

#remove the top key words and then plot words
bing_word_counts %>%
  filter(n > 20) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Sentiment")
```


#Map with sentiment

 Having calculated sentiment scores, the question is whether there is relationship between the intensity of sentiment and people's location, i.e. people live in the west area like California may have stronger sentiment than people from the east coast. To visaulize this question, below are the maps of sentimnet score for the total data and three set seperately. Here red represents positive sentiment and blue represent regative sentiment.
 There seems to be more users located on the east coast than on the west coast. Also, the color for tweets from the west coast is darker, especially in south California which means people there have more negative sentiment. While the reslut is in accordance with wordclouds that the sentiment under keyword "California fire" is more likely to be negative while under the hashtag #California wild fire is very positive.
 
```{r map sentiment, echo=FALSE, message=FALSE, warning=FALSE}
#Get the US map
#######If this line doesn't run or shows an error message, just run it again please, sometimes it happens but will eliminate after rerunning it#############
USmap = ggmap(get_googlemap(center =as.numeric(geocode("United States")),
                            scale=2, zoom=4), extent="device") 

#Draw map for total data
map1<- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((total$score>=0)),"red", "blue"), data=total, alpha=0.4, size=total$absolute_score) +
  scale_size_continuous(range=total$score)+
  ggtitle("Sentiment map for total data set")
map1
```

```{r map2, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under #Californiafire
map2 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data1$score>=0)),"red", "blue"), data=data1, alpha=0.4, size=data1$absolute_score) +
  scale_size_continuous(range=data1$score)+
  ggtitle("Sentiment map for #Californiafire")
map2
```

```{r map3, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under 'California fire'
map3 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data2$score>=0)),"red", "blue"), data=data2, alpha=0.4, size=data2$absolute_score) +
  scale_size_continuous(range=data2$score)+
  ggtitle("Sentiment map for 'California fire'")
map3
```


```{r map4, echo=FALSE, message=FALSE, warning=FALSE}
# mapping under #California wild fire
map4 <- USmap +
  geom_point(aes(x=lon, y=lat), col=ifelse(((data3$score>=0)),"red", "blue"), data=data3, alpha=0.4, size=data3$absolute_score) +
  scale_size_continuous(range=data3$score)+
  ggtitle("Sentiment map for #California wild fire")
map4
```

##Shiny Interactive Map

 I created an interactive map using Shiny application. As we already seen the sentiment scroe on the map, the interactive map focus on the number of retweets counts of each tweets to generate the popularity of the twitter. In the map, the deeper the color of the popup points, the more popular the tweet content is.
  Once you get access to the map, the shiny feature enable you to explore the data based on your preference. You can zoom the scale of the map and click on every individual points to find out more detail about that tweet, like sentiment socre, user name and retweet cout. Also, you can discover whether retweet counts is related to position of the user. Upgraded from ggmap experience, Shiny offers a great opportunities for better data visualization and interaction. Below is the link for the application:
###https://sabrina414.shinyapps.io/InteractiveMap/
```{r shiny, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Shiny interactive map for retweet count
d <- read.csv("total.csv", row.names = 1)
d <- mutate(d, popup_info=paste(sep = "<br/>", paste0("<b>",d$screenName, "</b>"), paste0 ("retweet count: ", d$retweetCount), paste0 ("sentiment score: ",d$score)))

factorpal<- colorFactor(
  palette = "RdPu",
  domain = c(d$retweetCount),
  level = NULL,
  ordered= FALSE, 
  na.color = "#808080"
)

r_colors <- rgb(t(col2rgb(colors()) / 255))
names(r_colors) <- colors()

ui <- fluidPage(
  leafletOutput("PopularityMap"),
  p()
)

server <- function(input, output, session) {
 
  output$PopularityMap <- renderLeaflet({
    leaflet(d) %>%
      addTiles(
      ) %>%  # Add default OpenStreetMap map tiles
      addCircleMarkers(lng=~lon,
                       lat = ~lat, 
                       popup= ~popup_info,
                       radius = 3,
                       color = ~factorpal(d$retweetCount),
                       fillOpacity = 1) %>%
      addProviderTiles("Stamen.Watercolor") %>%
      addProviderTiles("CartoDB.Positron") %>%
      setView(lng = -96.71289, lat = 37.09024, zoom = 4)
  })
}

shinyApp(ui, server)

```

#Statistical Analysis

 Having virsualize the data, here are some statistical analysis to support the findings above.
 

###Summary table
```{r score summary, echo=FALSE, message=FALSE, warning=FALSE}
summary(total$score)
```
 From the summary of score we can see the average sentiment score is negative, with minimum of -4 and maximum of 5. This means the overall sentiment is more negative, which is the same conclusion as above.
 

###Test of normality

```{r normal analyze, echo=FALSE, message=FALSE, warning=FALSE}

#draw histogram to see whether it follows normal distribution
hist(total$score)
```


The histogram shows that the data is approximately normal distributed



###ANOVA table
  
  Below is the Anova table that analyze the relationship between sentiment and retweet number. Here p-value<0.05, which means that we should reject the null hypothesis at 95% confidence interval. Then the conclusion is that sentiment score  have effect on retweet count, the stronger sentiment is, the more retweet count it would cause.
```{r anova, echo=FALSE, message=FALSE, warning=FALSE}
# Anova table that analyze the relationship between sentiment and retweet number
summary(lm(total$retweetCount~total$absolute_score))

```



The smooth line confirms the conclusion above. However, the dramatic trend may indicate specific relationship between retweet count and sentiment score, which need future investigation.
```{r, echo=FALSE, message=FALSE, warning=FALSE}

#Smooth trend
ggplot(total) +
  geom_smooth(mapping = aes(x = total$score, y = total$retweetCount))+
  ggtitle("Smooth Line for Retweet count v.s Sentiment score") + 
  xlab("Sentiment score") +
  ylab("Retweet count")
```



Is location influential on the strength of attitude?  Both latitude and longitude are highly insignificant in the anova table below, which means that the strength of attitude is not so related with people's location.

```{r, echo=FALSE}
summary(lm(total$absolute_score~total$lat+total$lon))
```


#Conclusion and Future Improvement
  From the analysis above, we can see that the sentiment of texts is different among different keywords and hashtags. Overall the data set is more negative. Also, the popularity of the text, represented by retweet counts, is related to the strength of attitude. However, there might be some repetitive texts among these three dataset since the keywords and hashtags are quite similar. For future improvement, these repetitive texts should be removed. Also, it would be better to overcome the restrict of Google geocode API and gather more data. A larger dataset will saturate this project with more factual evidences.